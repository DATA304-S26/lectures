{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "375c46ca",
   "metadata": {},
   "source": [
    "\n",
    "# DATA 304 â€” Module 5, Session 1\n",
    "## HTML and Web Scraping Demo\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Basics of HTTP requests\n",
    "- Parsing HTML with BeautifulSoup\n",
    "- Extracting tables with `pandas.read_html`\n",
    "- Parsing semi-structured content (div/span listings)\n",
    "- A small activity for practice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a885a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports\n",
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "# Core libs for this session\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import requests\n",
    "\n",
    "# Utility\n",
    "from io import StringIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7617bf",
   "metadata": {},
   "source": [
    "\n",
    "## 1) HTTP Requests Pattern\n",
    "\n",
    "Typical pattern when fetching a page:\n",
    "1. Make a `GET` request.\n",
    "2. Check status code.\n",
    "3. Use `response.text` as the HTML to parse.\n",
    "\n",
    "Below we **show** the pattern. The environment here has no internet, so the actual request is wrapped in a try/except and falls back to a local HTML sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b953cc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# URL\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_FIFA_World_Cup_finals\"\n",
    "\n",
    "try:\n",
    "    resp = requests.get(url, timeout=10)\n",
    "    resp.raise_for_status()\n",
    "    html = resp.text\n",
    "    print(\"Fetched from the web:\", url)\n",
    "except Exception as e:\n",
    "    print(\"Falling back to local sample HTML due to:\", e.__class__.__name__)\n",
    "    html2 = '\\n<!DOCTYPE html>\\n<html>\\n  <head>\\n    <title>Demo Page</title>\\n  </head>\\n  <body>\\n    <h1 class=\"title\">Sample Headline</h1>\\n    <p id=\"msg\">Hello, world.</p>\\n\\n    <h2>Top Stories</h2>\\n    <ul>\\n      <li><a href=\"/story/1\">Story One</a></li>\\n      <li><a href=\"/story/2\">Story Two</a></li>\\n      <li><a href=\"/story/3\">Story Three</a></li>\\n    </ul>\\n\\n    <h2>Population Table</h2>\\n    <table>\\n      <thead>\\n        <tr><th>Country</th><th>Population</th></tr>\\n      </thead>\\n      <tbody>\\n        <tr><td>Aland</td><td>30,000</td></tr>\\n        <tr><td>Bravo</td><td>1,250,000</td></tr>\\n        <tr><td>Charlie</td><td>9,999,999</td></tr>\\n      </tbody>\\n    </table>\\n\\n    <h2>Products</h2>\\n    <div class=\"product\">\\n      <span class=\"name\">Widget A</span>\\n      <span class=\"price\">$9.99</span>\\n    </div>\\n    <div class=\"product promo\">\\n      <span class=\"name\">Widget B</span>\\n      <span class=\"price\">$14.50</span>\\n    </div>\\n  </body>\\n</html>\\n'  # local demo HTML string\n",
    "    print(\"Using local sample HTML string instead.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273568b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_FIFA_World_Cup_finals\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "resp = requests.get(url, headers=headers, timeout=10)\n",
    "resp.raise_for_status()\n",
    "html = resp.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01be8e6a",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Parsing HTML with BeautifulSoup\n",
    "\n",
    "We create a `BeautifulSoup` object and then query elements by tag, attribute, or CSS selectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ecd898",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Extract title text\n",
    "page_title = soup.title.text if soup.title else None\n",
    "print(\"Page title:\", page_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c3aa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: get the H1 with class 'title'\n",
    "h1_title = soup.find(\"h1\", {\"class\": \"header\"})\n",
    "print(\"H1 .title ->\", h1_title.text if h1_title else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502ee87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract story links under 'Top Stories'\n",
    "links = [(a.text.strip(), a.get(\"href\")) for a in soup.select(\"ul li a\")]\n",
    "links[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133c75ca",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Extracting Tables with `pandas.read_html`\n",
    "\n",
    "`pandas.read_html` can parse one or more tables from a page or from an HTML string. It returns a list of DataFrames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14106f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use StringIO to provide the HTML string as a file-like object\n",
    "tables = pd.read_html(StringIO(html))  # requires lxml or html5lib installed\n",
    "print(f\"Found {len(tables)} table(s).\")\n",
    "tables[0].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e38fb62",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Semi-Structured Content: Product Listings\n",
    "\n",
    "Many sites use `<div>`/`<span>` structures rather than `<table>`. We can extract and normalize these into a DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c0e680",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2 = BeautifulSoup(html2, \"html.parser\")\n",
    "products = []\n",
    "for card in soup2.select(\"div.product\"):\n",
    "    name = card.find(\"span\", class_=\"name\")\n",
    "    price = card.find(\"span\", class_=\"price\")\n",
    "    products.append({\n",
    "        \"name\": name.text.strip() if name else None,\n",
    "        \"price_raw\": price.text.strip() if price else None,\n",
    "        \"is_promo\": \"promo\" in (card.get(\"class\") or []),\n",
    "    })\n",
    "\n",
    "df_products = pd.DataFrame(products)\n",
    "df_products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30520f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clean the price column into numeric where possible\n",
    "def parse_price(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    x = x.replace(\"$\", \"\").replace(\",\", \"\").strip()\n",
    "    try:\n",
    "        return float(x)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "df_products[\"price\"] = df_products[\"price_raw\"].map(parse_price)\n",
    "df_products.drop(columns=[\"price_raw\"], inplace=True)\n",
    "df_products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbf54de",
   "metadata": {},
   "outputs": [],
   "source": [
    "url3 = \"https://books.toscrape.com\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "resp3 = requests.get(url3, headers=headers, timeout=10)\n",
    "resp3.raise_for_status()\n",
    "html3 = resp3.text\n",
    "soup3 = BeautifulSoup(html3, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2e44af",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for card in soup3.select(\"article.product_pod\"):\n",
    "    a = card.select_one(\"h3 a\")\n",
    "    rows.append({\n",
    "        \"name\": a.get(\"title\"),\n",
    "        \"price_raw\": card.select_one(\"p.price_color\").text.strip(),\n",
    "        \"in_stock\": \"In stock\" in card.select_one(\"p.instock.availability\").text,\n",
    "        \"rating\": next((c for c in card.select_one(\"p.star-rating\").get(\"class\", []) if c != \"star-rating\"), None),\n",
    "        \"url\": requests.compat.urljoin(url3, a.get(\"href\")),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe237cb",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Summary\n",
    "\n",
    "- Use `requests` to fetch pages. Check status codes. Respect site policies.\n",
    "- Parse HTML with BeautifulSoup. Use `find`, `find_all`, and CSS selectors.\n",
    "- Use `pandas.read_html` for HTML tables when available.\n",
    "- For semi-structured content, select container elements and normalize to a DataFrame.\n",
    "- Always validate and clean extracted data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
